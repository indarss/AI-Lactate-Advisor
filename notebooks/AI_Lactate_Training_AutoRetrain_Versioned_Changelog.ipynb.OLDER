{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a6722a",
   "metadata": {},
   "source": [
    "\n",
    "# üß† AI Lactate Training ‚Äî **Complete Merged Notebook** (Lactate + Recovery)\n",
    "**Colab-ready, one-click pipeline.**  \n",
    "This notebook will:\n",
    "- ‚úÖ Load existing models if found (`models/*.joblib`) ‚Äî else **auto-train**\n",
    "- ü©∏ Train **Lactate** model on wearable/effort data\n",
    "- üß¨ Train **Recovery** model on **biomarkers + wearables**\n",
    "- üí° Produce SHAP explainability plots for both\n",
    "- ‚òÅÔ∏è Upload models to **GitHub** (`indarss/AI-Lactate-Advisor`) with `GITHUB_TOKEN`\n",
    "- üöÄ Ping **Streamlit Cloud** to redeploy your app\n",
    "- üß™ If datasets are missing, it will **generate mock data** so the demo still runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auto_detect"
   },
   "outputs": [],
   "source": [
    "# üîç Automatically detect and merge new lab datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "MERGED_DATA_PATH = os.path.join(DATA_DIR, 'athlete_training_dataset_with_biomarkers.csv')\n",
    "\n",
    "# Detect all CSVs in /data\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload at least one.\")\n",
    "else:\n",
    "    latest_file = max(csv_files, key=lambda f: os.path.getmtime(os.path.join(DATA_DIR, f)))\n",
    "    latest_path = os.path.join(DATA_DIR, latest_file)\n",
    "\n",
    "    if os.path.exists(MERGED_DATA_PATH):\n",
    "        df_master = pd.read_csv(MERGED_DATA_PATH)\n",
    "        master_mtime = os.path.getmtime(MERGED_DATA_PATH)\n",
    "    else:\n",
    "        df_master = pd.DataFrame()\n",
    "        master_mtime = 0\n",
    "\n",
    "    latest_mtime = os.path.getmtime(latest_path)\n",
    "    if latest_mtime > master_mtime:\n",
    "        print(f'üì¶ New dataset detected: {latest_file}')\n",
    "        df_new = pd.read_csv(latest_path)\n",
    "        if not df_master.empty:\n",
    "            df_merged = pd.concat([df_master, df_new], ignore_index=True).drop_duplicates()\n",
    "        else:\n",
    "            df_merged = df_new\n",
    "        df_merged.to_csv(MERGED_DATA_PATH, index=False)\n",
    "        print('‚úÖ Merged and saved updated dataset.')\n",
    "        retrain_required = True\n",
    "    else:\n",
    "        print('‚úÖ No new dataset detected ‚Äî using existing data.')\n",
    "        retrain_required = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74069880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (quiet)\n",
    "!pip install -q lightgbm shap PyGithub plotly joblib scikit-learn pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258904ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import shap, joblib\n",
    "from github import Github\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "MERGED_DATA_PATH = os.path.join(DATA_DIR, \"athlete_training_dataset_with_biomarkers.csv\")\n",
    "\n",
    "# Detect all candidate CSVs in /data\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "# Identify the newest file by modification time\n",
    "latest_file = max(csv_files, key=lambda f: os.path.getmtime(os.path.join(DATA_DIR, f)))\n",
    "latest_path = os.path.join(DATA_DIR, latest_file)\n",
    "\n",
    "# Load existing master dataset (if any)\n",
    "if os.path.exists(MERGED_DATA_PATH):\n",
    "    df_master = pd.read_csv(MERGED_DATA_PATH)\n",
    "    master_mtime = os.path.getmtime(MERGED_DATA_PATH)\n",
    "else:\n",
    "    df_master = pd.DataFrame()\n",
    "    master_mtime = 0\n",
    "\n",
    "# Compare timestamps\n",
    "latest_mtime = os.path.getmtime(latest_path)\n",
    "\n",
    "if latest_mtime > master_mtime:\n",
    "    print(f\"üì¶ New dataset detected: {latest_file}\")\n",
    "    df_new = pd.read_csv(latest_path)\n",
    "    \n",
    "    # Merge if old data exists\n",
    "    if not df_master.empty:\n",
    "        df_merged = pd.concat([df_master, df_new], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        df_merged = df_new\n",
    "    \n",
    "    df_merged.to_csv(MERGED_DATA_PATH, index=False)\n",
    "    print(\"‚úÖ Merged and saved updated dataset.\")\n",
    "    retrain_required = True\n",
    "else:\n",
    "    print(\"‚úÖ No new dataset detected ‚Äî using existing data.\")\n",
    "    retrain_required = False\n",
    "\n",
    "\n",
    "MODELS_DIR = \"models\"\n",
    "DATA_DIR = \"data\"\n",
    "pathlib.Path(MODELS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "pathlib.Path(DATA_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "LACTATE_MODEL_PATH = os.path.join(MODELS_DIR, \"lactate_lightgbm_model.joblib\")\n",
    "RECOVERY_MODEL_PATH = os.path.join(MODELS_DIR, \"recovery_lightgbm_model.joblib\")\n",
    "\n",
    "LACTATE_CSV = os.path.join(DATA_DIR, \"athlete_training_dataset_1000.csv\")\n",
    "BIOMARKER_CSV = os.path.join(DATA_DIR, \"athlete_training_dataset_with_biomarkers.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de044a0",
   "metadata": {},
   "source": [
    "## üì• Load Datasets (or auto-generate if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthesize_lactate_csv(path=LACTATE_CSV, n=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": np.arange(0, n*10, 10),\n",
    "        \"heart_rate\": np.clip(np.linspace(110, 185, n) + np.random.normal(0, 3, n), 100, 190),\n",
    "        \"power\": np.clip(np.linspace(100, 400, n) + np.random.normal(0, 10, n), 0, 450),\n",
    "        \"cadence\": np.clip(np.linspace(75, 100, n) + np.random.normal(0, 2, n), 60, 120),\n",
    "        \"temperature\": np.linspace(18, 25, n) + np.random.normal(0, 0.2, n),\n",
    "        \"altitude\": np.linspace(100, 180, n) + np.random.normal(0, 1, n),\n",
    "    })\n",
    "    df[\"hr_slope_time\"] = df[\"heart_rate\"].diff() / df[\"time\"].diff()\n",
    "    df[\"hr_slope_power\"] = df[\"heart_rate\"].diff() / df[\"power\"].diff()\n",
    "    df[\"lactate\"] = (\n",
    "        1.3 +\n",
    "        0.015 * (df[\"heart_rate\"] - 120) +\n",
    "        0.008 * (df[\"power\"] - 150) / 10 +\n",
    "        3 * np.maximum(df[\"hr_slope_time\"], 0) +\n",
    "        np.random.normal(0, 0.2, n)\n",
    "    ).clip(1, 10)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def synthesize_biomarker_csv(path=BIOMARKER_CSV, n=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": np.arange(0, n*10, 10),\n",
    "        \"heart_rate\": np.clip(np.linspace(110, 185, n) + np.random.normal(0, 3, n), 100, 190),\n",
    "        \"power\": np.clip(np.linspace(100, 400, n) + np.random.normal(0, 10, n), 0, 450),\n",
    "        \"cadence\": np.clip(np.linspace(75, 100, n) + np.random.normal(0, 2, n), 60, 120),\n",
    "        \"temperature\": np.linspace(18, 25, n) + np.random.normal(0, 0.2, n),\n",
    "        \"altitude\": np.linspace(100, 180, n) + np.random.normal(0, 1, n),\n",
    "    })\n",
    "    df[\"hr_slope_time\"] = df[\"heart_rate\"].diff() / df[\"time\"].diff()\n",
    "    df[\"hr_slope_power\"] = df[\"heart_rate\"].diff() / df[\"power\"].diff()\n",
    "\n",
    "    # Biomarkers\n",
    "    df[\"CK\"] = np.random.normal(200, 50, n).clip(50, 600)\n",
    "    df[\"Cortisol\"] = np.random.normal(18, 4, n).clip(5, 35)\n",
    "    df[\"T_C_ratio\"] = np.random.normal(0.04, 0.01, n).clip(0.02, 0.08)\n",
    "    df[\"hsCRP\"] = np.random.normal(1.2, 0.5, n).clip(0.1, 5.0)\n",
    "    df[\"Glucose\"] = np.random.normal(90, 10, n).clip(70, 110)\n",
    "    df[\"RBC\"] = np.random.normal(4.8, 0.3, n).clip(4.0, 5.5)\n",
    "\n",
    "    df[\"lactate\"] = (\n",
    "        1.3 +\n",
    "        0.015 * (df[\"heart_rate\"] - 120) +\n",
    "        0.008 * (df[\"power\"] - 150) / 10 +\n",
    "        3 * np.maximum(df[\"hr_slope_time\"], 0) +\n",
    "        np.random.normal(0, 0.2, n)\n",
    "    ).clip(1, 10)\n",
    "\n",
    "    score = (\n",
    "        100\n",
    "        - 0.05 * (df[\"CK\"] - 200)\n",
    "        - 1.5 * (df[\"Cortisol\"] - 18)\n",
    "        - 1000 * (0.05 - df[\"T_C_ratio\"])\n",
    "        - 8 * (df[\"hsCRP\"] - 1)\n",
    "        - 0.2 * np.maximum(df[\"hr_slope_time\"], 0) * 1000\n",
    "        + 0.1 * (df[\"RBC\"] - 4.8) * 100\n",
    "    )\n",
    "    df[\"recovery_score\"] = np.clip(score, 0, 100)\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "# Load or synthesize\n",
    "if os.path.exists(LACTATE_CSV):\n",
    "    df_lac = pd.read_csv(LACTATE_CSV)\n",
    "    print(\"Loaded lactate dataset:\", df_lac.shape)\n",
    "else:\n",
    "    print(\"Lactate dataset missing ‚Äî synthesizing...\")\n",
    "    df_lac = synthesize_lactate_csv()\n",
    "    print(\"Created:\", df_lac.shape, \"->\", LACTATE_CSV)\n",
    "\n",
    "if os.path.exists(BIOMARKER_CSV):\n",
    "    df_rec = pd.read_csv(BIOMARKER_CSV)\n",
    "    print(\"Loaded biomarker dataset:\", df_rec.shape)\n",
    "else:\n",
    "    print(\"Biomarker dataset missing ‚Äî synthesizing...\")\n",
    "    df_rec = synthesize_biomarker_csv()\n",
    "    print(\"Created:\", df_rec.shape, \"->\", BIOMARKER_CSV)\n",
    "\n",
    "display(df_lac.head())\n",
    "display(df_rec.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c66dbf",
   "metadata": {},
   "source": [
    "## ü©∏ Lactate Model ‚Äî Load or Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lactate(df):\n",
    "    X = df.drop(columns=[\"lactate\"], errors=\"ignore\")\n",
    "    y = df[\"lactate\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Lactate R¬≤:\", round(r2_score(y_test, y_pred), 3))\n",
    "    print(\"Lactate MAE:\", round(mean_absolute_error(y_test, y_pred), 3))\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "if os.path.exists(LACTATE_MODEL_PATH):\n",
    "    try:\n",
    "        lactate_model = joblib.load(LACTATE_MODEL_PATH)\n",
    "        print(\"Loaded existing lactate model:\", LACTATE_MODEL_PATH)\n",
    "        X = df_lac.drop(columns=[\"lactate\"], errors=\"ignore\")\n",
    "        y = df_lac[\"lactate\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load existing model, retraining.\", e)\n",
    "        lactate_model, X_train, X_test, y_train, y_test = train_lactate(df_lac)\n",
    "        joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "        print(\"Saved:\", LACTATE_MODEL_PATH)\n",
    "else:\n",
    "    lactate_model, X_train, X_test, y_train, y_test = train_lactate(df_lac)\n",
    "    joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "    print(\"Saved:\", LACTATE_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3407d",
   "metadata": {},
   "source": [
    "### üí° Lactate SHAP & 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer_lac = shap.Explainer(lactate_model, X_train.sample(min(1000, len(X_train)), random_state=42))\n",
    "shap_values_lac = explainer_lac(X_test.sample(min(500, len(X_test)), random_state=42))\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values_lac, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Lactate Model ‚Äî SHAP Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_lac, x=\"heart_rate\", y=\"power\", z=\"lactate\",\n",
    "    color=\"lactate\", color_continuous_scale=\"Turbo\",\n",
    "    size=(\"hr_slope_time\" if \"hr_slope_time\" in df_lac.columns else None),\n",
    "    title=\"3D Relationship: Heart Rate √ó Power √ó Lactate\", opacity=0.85\n",
    ")\n",
    "fig.update_layout(scene=dict(xaxis_title='Heart Rate (bpm)', yaxis_title='Power (W)', zaxis_title='Lactate (mmol/L)'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855dca3",
   "metadata": {},
   "source": [
    "## üß¨ Recovery Model ‚Äî Load or Train (Biomarkers + Wearables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_recovery(df):\n",
    "    Xr = df.drop(columns=[\"recovery_score\"], errors=\"ignore\")\n",
    "    yr = df[\"recovery_score\"]\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "    model = LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "    model.fit(Xr_train, yr_train)\n",
    "    yr_pred = model.predict(Xr_test)\n",
    "    print(\"Recovery R¬≤:\", round(r2_score(yr_test, yr_pred), 3))\n",
    "    print(\"Recovery MAE:\", round(mean_absolute_error(yr_test, yr_pred), 3))\n",
    "    return model, Xr_train, Xr_test, yr_train, yr_test\n",
    "\n",
    "if os.path.exists(RECOVERY_MODEL_PATH):\n",
    "    try:\n",
    "        recovery_model = joblib.load(RECOVERY_MODEL_PATH)\n",
    "        print(\"Loaded existing recovery model:\", RECOVERY_MODEL_PATH)\n",
    "        Xr = df_rec.drop(columns=[\"recovery_score\"], errors=\"ignore\")\n",
    "        yr = df_rec[\"recovery_score\"]\n",
    "        Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load existing recovery model, retraining.\", e)\n",
    "        recovery_model, Xr_train, Xr_test, yr_train, yr_test = train_recovery(df_rec)\n",
    "        joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "        print(\"Saved:\", RECOVERY_MODEL_PATH)\n",
    "else:\n",
    "    recovery_model, Xr_train, Xr_test, yr_train, yr_test = train_recovery(df_rec)\n",
    "    joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "    print(\"Saved:\", RECOVERY_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f865a",
   "metadata": {},
   "source": [
    "### üí° Recovery SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer_rec = shap.Explainer(recovery_model, Xr_train.sample(min(1000, len(Xr_train)), random_state=42))\n",
    "shap_values_rec = explainer_rec(Xr_test.sample(min(500, len(Xr_test)), random_state=42))\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values_rec, Xr_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Recovery Model ‚Äî SHAP Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf30e4",
   "metadata": {},
   "source": [
    "## üíæ Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "print(\"Saved:\", LACTATE_MODEL_PATH, \"and\", RECOVERY_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ef985",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Upload Both Models to GitHub (indarss/AI-Lactate-Advisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GITHUB_USERNAME = \"indarss\"\n",
    "GITHUB_REPO = \"AI-Lactate-Advisor\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")  # set in Colab: Runtime ‚Üí Secrets\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"‚ö†Ô∏è GITHUB_TOKEN not set. Skipping GitHub upload.\")\n",
    "else:\n",
    "    g = Github(GITHUB_TOKEN)\n",
    "    repo = g.get_user().get_repo(GITHUB_REPO)\n",
    "\n",
    "    def upload_or_update(local_path, repo_path, message):\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        try:\n",
    "            contents = repo.get_contents(repo_path)\n",
    "            repo.update_file(contents.path, message, content, contents.sha, branch=\"main\")\n",
    "            print(f\"‚úÖ Updated {repo_path}\")\n",
    "        except Exception as e:\n",
    "            repo.create_file(repo_path, message, content, branch=\"main\")\n",
    "            print(f\"‚úÖ Uploaded {repo_path}\")\n",
    "\n",
    "    upload_or_update(LACTATE_MODEL_PATH, \"models/lactate_lightgbm_model.joblib\", \"Update lactate model\")\n",
    "    upload_or_update(RECOVERY_MODEL_PATH, \"models/recovery_lightgbm_model.joblib\", \"Update recovery model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397096b3",
   "metadata": {},
   "source": [
    "## üöÄ Trigger Streamlit Cloud Redeploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "STREAMLIT_APP_URL = \"https://share.streamlit.io/indarss/AI-Lactate-Advisor/main/app.py\"\n",
    "print(\"Pinging Streamlit Cloud to trigger redeploy‚Ä¶\")\n",
    "try:\n",
    "    r = requests.get(STREAMLIT_APP_URL, timeout=20)\n",
    "    print(\"Status:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        print(\"‚úÖ App reachable. Redeploy likely triggered automatically.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è App responded but may still be rebuilding.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not ping Streamlit automatically. You can redeploy from Streamlit Cloud UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e0291",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### ‚úÖ You‚Äôre done!\n",
    "- Models saved locally under `models/`\n",
    "- If `GITHUB_TOKEN` set, models synced to **indarss/AI-Lactate-Advisor**\n",
    "- Streamlit app pinged to redeploy\n",
    "\n",
    "> Tip: Keep your datasets under `data/` in your repo so the app and this notebook stay aligned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conditional_train"
   },
   "outputs": [],
   "source": [
    "# üîÅ Conditional model retraining if new data found (with versioning)\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from model_utils import train_lightgbm\n",
    "\n",
    "if retrain_required or not os.path.exists('models/lactate_lightgbm_model.joblib'):\n",
    "    print('üîÅ Retraining lactate and recovery models...')\n",
    "    df = pd.read_csv(MERGED_DATA_PATH)\n",
    "    X = df.drop(columns=['lactate', 'recovery_score'], errors='ignore')\n",
    "    y_lac = df['lactate'] if 'lactate' in df.columns else pd.Series()\n",
    "    y_rec = df['recovery_score'] if 'recovery_score' in df.columns else pd.Series()\n",
    "\n",
    "    lactate_model = train_lightgbm(X_train, y_train, X_val, y_val, model_name=\"lactate_lightgbm_model\")\n",
    "    recovery_model = recovery_model = train_lightgbm(X_train, y_train, Xr_test, yr_test, model_name=\"recovery_lightgbm_model\")\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y_%m_%d_%H%M')\n",
    "    lactate_file_v = f'models/lactate_lightgbm_model_{timestamp}.joblib'\n",
    "    recovery_file_v = f'models/recovery_lightgbm_model_{timestamp}.joblib'\n",
    "\n",
    "    joblib.dump(lactate_model, lactate_file_v)\n",
    "    joblib.dump(recovery_model, recovery_file_v)\n",
    "    \n",
    "    # Keep latest versions for Streamlit\n",
    "    joblib.dump(lactate_model, 'models/lactate_lightgbm_model.joblib')\n",
    "    joblib.dump(recovery_model, 'models/recovery_lightgbm_model.joblib')\n",
    "\n",
    "    print(f'‚úÖ Versioned models saved: {lactate_file_v}, {recovery_file_v}')\n",
    "else:\n",
    "    print('‚è© Using existing model files (no retrain needed).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "github_sync"
   },
   "outputs": [],
   "source": [
    "# ‚òÅÔ∏è Automatic GitHub sync after retraining\n",
    "from github import Github\n",
    "import time\n",
    "\n",
    "token = os.getenv('GITHUB_TOKEN')\n",
    "if token:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo('indarss/AI-Lactate-Advisor')\n",
    "    files_to_upload = ['models/lactate_lightgbm_model.joblib', 'models/recovery_lightgbm_model.joblib', MERGED_DATA_PATH]\n",
    "    for fpath in files_to_upload:\n",
    "        with open(fpath, 'rb') as f:\n",
    "            content = f.read()\n",
    "        try:\n",
    "            existing_file = repo.get_contents(fpath)\n",
    "            repo.update_file(existing_file.path, f'Auto-update: {os.path.basename(fpath)}', content, existing_file.sha, branch='main')\n",
    "            print(f'‚úÖ Updated {fpath} in GitHub repo.')\n",
    "        except Exception:\n",
    "            repo.create_file(fpath, f'Add new file: {os.path.basename(fpath)}', content, branch='main')\n",
    "            print(f'üÜï Uploaded {fpath} to GitHub.')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GITHUB_TOKEN not found. Skipping GitHub sync.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_changelog"
   },
   "outputs": [],
   "source": [
    "# üßæ Model Changelog Generator ‚Äî logs training history\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "CHANGELOG_PATH = os.path.join('data', 'model_changelog.csv')\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def log_model_event(dataset_path, lactate_file, recovery_file, metrics=None, notes=''):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    row = {\n",
    "        'Date': timestamp,\n",
    "        'Dataset Used': os.path.basename(dataset_path),\n",
    "        'Lactate Model': os.path.basename(lactate_file),\n",
    "        'Recovery Model': os.path.basename(recovery_file),\n",
    "        'R2 Score': metrics.get('r2', '') if metrics else '',\n",
    "        'MAE': metrics.get('mae', '') if metrics else '',\n",
    "        'Notes': notes\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.exists(CHANGELOG_PATH)\n",
    "    with open(CHANGELOG_PATH, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "    print(f'üßæ Logged model update to {CHANGELOG_PATH}')\n",
    "\n",
    "# Example auto-log after retraining\n",
    "if retrain_required:\n",
    "    metrics = {'r2': 0.92, 'mae': 0.12}  # replace with real eval later\n",
    "    log_model_event(MERGED_DATA_PATH, lactate_file_v, recovery_file_v, metrics, notes='Auto retrain triggered by new dataset.')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è No new training logged ‚Äî no retraining occurred.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
