{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4811109",
   "metadata": {},
   "source": [
    "# üß† AI Lactate Advisor ‚Äì Final Training Notebook (Merged & Clean)\n",
    "\n",
    "This notebook trains and versions two models:\n",
    "\n",
    "1. **Lactate Model** ‚Äì predicts blood lactate (mmol/L) from time-series features  \n",
    "2. **Recovery Model** ‚Äì predicts recovery/readiness score from biomarker data  \n",
    "\n",
    "It is designed to be consistent with the **Streamlit app**, saving models into `models/`\n",
    "and appending results to `models/training_log.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a670181",
   "metadata": {},
   "source": [
    "## üìò Cell 1 ‚Äî Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üìò Cell 1 ‚Äî Imports & Global Config\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Install PyGithub if not already installed\n",
    "try:\n",
    "    from github import Github\n",
    "except ImportError:\n",
    "    print(\"Installing PyGithub...\")\n",
    "    !pip install PyGithub\n",
    "    from github import Github  # optional; ok if not used\n",
    "\n",
    "\n",
    "from model_utils import (\n",
    "    add_hr_slopes,\n",
    "    add_rolling_features,\n",
    ")\n",
    "\n",
    "# Paths relative to repo root\n",
    "DATA_DIR = \"data\"\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "LACTATE_MODEL_PATH = os.path.join(MODELS_DIR, \"lactate_lightgbm_model.joblib\")\n",
    "RECOVERY_MODEL_PATH = os.path.join(MODELS_DIR, \"recovery_lightgbm_model.joblib\")\n",
    "\n",
    "print(\"üìÅ DATA_DIR:\", DATA_DIR)\n",
    "print(\"üìÅ MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0782c99",
   "metadata": {},
   "source": [
    "## üìó Cell 2 ‚Äî Load & Merge Data (Auto-Retrain Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üìó Cell 2 ‚Äî Load & Merge Data (Auto-Retrain Logic)\n",
    "# =============================================================\n",
    "\n",
    "MERGED_DATASET = os.path.join(DATA_DIR, \"merged_training_data.csv\")\n",
    "\n",
    "# If merged file exists, use it as \"master\"\n",
    "if os.path.exists(MERGED_DATASET):\n",
    "    df_master = pd.read_csv(MERGED_DATASET)\n",
    "    master_mtime = os.path.getmtime(MERGED_DATASET)\n",
    "else:\n",
    "    df_master = pd.DataFrame()\n",
    "    master_mtime = 0\n",
    "\n",
    "# Find all CSVs in data/ that look like training data\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"‚ùå No CSV files in data/ folder.\")\n",
    "\n",
    "latest_file = max(csv_files, key=lambda f: os.path.getmtime(os.path.join(DATA_DIR, f)))\n",
    "latest_path = os.path.join(DATA_DIR, latest_file)\n",
    "latest_mtime = os.path.getmtime(latest_path)\n",
    "\n",
    "print(\"üìÑ Latest CSV:\", latest_file)\n",
    "\n",
    "if latest_mtime > master_mtime:\n",
    "    print(\"üì¶ Newer dataset detected ‚Üí merging into merged_training_data.csv\")\n",
    "    df_new = pd.read_csv(latest_path)\n",
    "    if not df_master.empty:\n",
    "        df_merged = pd.concat([df_master, df_new], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        df_merged = df_new\n",
    "    df_merged.to_csv(MERGED_DATASET, index=False)\n",
    "    df_all = df_merged\n",
    "else:\n",
    "    print(\"‚úÖ No newer CSV; using existing merged_training_data.csv\")\n",
    "    df_all = pd.read_csv(MERGED_DATASET)\n",
    "\n",
    "print(\"üìä Merged dataset shape:\", df_all.shape)\n",
    "df_all.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb5dbd",
   "metadata": {},
   "source": [
    "##  üìó Cell 3 ‚Äî Feature Engineering for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üìó Cell 3 ‚Äî Feature Engineering (Lactate + Recovery)\n",
    "# =============================================================\n",
    "\n",
    "df = df_all.copy()\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"lactate\", \"recovery_score\", \"hr\", \"power\"]:\n",
    "    if col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: column '{col}' missing in merged data.\")\n",
    "\n",
    "assert \"lactate\" in df.columns, \"‚ùå 'lactate' column required in merged_training_data.csv\"\n",
    "assert \"recovery_score\" in df.columns, \"‚ùå 'recovery_score' column required.\"\n",
    "\n",
    "# Rename hr ‚Üí heart_rate for consistency with add_hr_slopes\n",
    "if \"hr\" in df.columns:\n",
    "    df = df.rename(columns={\"hr\": \"heart_rate\"})\n",
    "\n",
    "# Apply your feature engineering pipeline\n",
    "df = add_hr_slopes(df)\n",
    "df = add_rolling_features(df, window=30)\n",
    "\n",
    "# Rename back to hr for app compatibility\n",
    "if \"heart_rate\" in df.columns:\n",
    "    df = df.rename(columns={\"heart_rate\": \"hr\"})\n",
    "\n",
    "print(\"‚úÖ Feature engineering complete. Columns now:\", len(df.columns))\n",
    "\n",
    "# --- Build lactate dataset ---\n",
    "df_lac = df.dropna(subset=[\"lactate\"]).copy()\n",
    "X_lac = df_lac.drop(columns=[\"lactate\", \"recovery_score\"], errors=\"ignore\")\n",
    "y_lac = df_lac[\"lactate\"]\n",
    "\n",
    "print(\"üìä Lactate X shape:\", X_lac.shape, \" y:\", y_lac.shape)\n",
    "\n",
    "# --- Build recovery dataset ---\n",
    "df_rec = df.dropna(subset=[\"recovery_score\"]).copy()\n",
    "# Use all numeric columns except labels as features\n",
    "numeric_cols = df_rec.select_dtypes(include=[np.number]).columns\n",
    "feature_cols_rec = [c for c in numeric_cols if c not in [\"lactate\", \"recovery_score\"]]\n",
    "\n",
    "X_rec = df_rec[feature_cols_rec]\n",
    "y_rec = df_rec[\"recovery_score\"]\n",
    "\n",
    "print(\"üìä Recovery X shape:\", X_rec.shape, \" y:\", y_rec.shape)\n",
    "print(\"üß¨ Recovery features:\", feature_cols_rec[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0d4a6",
   "metadata": {},
   "source": [
    "## üìó Cell 4 ‚Äî Train Both Models (with Feature Schema Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dafc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üìó Cell 4 ‚Äî Train Lactate & Recovery Models\n",
    "# =============================================================\n",
    "\n",
    "def train_lightgbm_model(X, y, name: str):\n",
    "    \"\"\"Train a LightGBM regressor and return model + metrics + feature list.\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"üöÄ Training {name} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    print(f\"üìà {name} VALID R¬≤ = {r2:.3f}\")\n",
    "    print(f\"üìâ {name} VALID MAE = {mae:.3f}\")\n",
    "\n",
    "    return model, r2, mae, list(X.columns)\n",
    "\n",
    "# ---- Lactate model ----\n",
    "lactate_model, r2_lac, mae_lac, lactate_features = train_lightgbm_model(X_lac, y_lac, \"Lactate\")\n",
    "\n",
    "# ---- Recovery model ----\n",
    "recovery_model, r2_rec, mae_rec, recovery_features = train_lightgbm_model(X_rec, y_rec, \"Recovery\")\n",
    "\n",
    "print(\"‚úÖ Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989f42d",
   "metadata": {},
   "source": [
    "## üìó Cell 5 ‚Äî Save Models (Wrapped with Schema) + Training Log + Optional GitHub Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üìó Cell 5 ‚Äî Save Models + Versioning + Log + Optional GitHub Upload\n",
    "# =============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Wrap models with feature schema\n",
    "lactate_wrapper = {\"model\": lactate_model, \"features\": lactate_features}\n",
    "recovery_wrapper = {\"model\": recovery_model, \"features\": recovery_features}\n",
    "\n",
    "# Save latest\n",
    "joblib.dump(lactate_wrapper, LACTATE_MODEL_PATH)\n",
    "joblib.dump(recovery_wrapper, RECOVERY_MODEL_PATH)\n",
    "\n",
    "# Save versioned copies\n",
    "ver_lac = os.path.join(MODELS_DIR, f\"lactate_lightgbm_model_{timestamp}.joblib\")\n",
    "ver_rec = os.path.join(MODELS_DIR, f\"recovery_lightgbm_model_{timestamp}.joblib\")\n",
    "joblib.dump(lactate_wrapper, ver_lac)\n",
    "joblib.dump(recovery_wrapper, ver_rec)\n",
    "\n",
    "print(\"üíæ Saved:\")\n",
    "print(\"  \", LACTATE_MODEL_PATH)\n",
    "print(\"  \", RECOVERY_MODEL_PATH)\n",
    "print(\"  \", ver_lac)\n",
    "print(\"  \", ver_rec)\n",
    "\n",
    "# ---- Training Log ----\n",
    "log_path = os.path.join(MODELS_DIR, \"training_log.csv\")\n",
    "log_entry = pd.DataFrame([{\n",
    "    \"timestamp\": timestamp,\n",
    "    \"r2_lactate\": r2_lac,\n",
    "    \"mae_lactate\": mae_lac,\n",
    "    \"r2_recovery\": r2_rec,\n",
    "    \"mae_recovery\": mae_rec,\n",
    "    \"rows\": len(df_all)\n",
    "}])\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    log_entry.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    log_entry.to_csv(log_path, index=False)\n",
    "\n",
    "print(f\"üìù Logged training metrics ‚Üí {log_path}\")\n",
    "\n",
    "# ---- Optional: GitHub upload (models + log) ----\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "GITHUB_USER = \"indarss\"\n",
    "GITHUB_REPO = \"AI-Lactate-Advisor\"\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    try:\n",
    "        g = Github(GITHUB_TOKEN)\n",
    "        repo = g.get_user().get_repo(GITHUB_REPO)\n",
    "\n",
    "        def upload_or_update(local_path, repo_path, message):\n",
    "            with open(local_path, \"rb\") as f:\n",
    "                content = f.read()\n",
    "            try:\n",
    "                existing = repo.get_contents(repo_path)\n",
    "                repo.update_file(existing.path, message, content, existing.sha, branch=\"main\")\n",
    "                print(f\"‚úÖ Updated {repo_path} on GitHub\")\n",
    "            except Exception:\n",
    "                repo.create_file(repo_path, message, content, branch=\"main\")\n",
    "                print(f\"‚úÖ Uploaded {repo_path} to GitHub\")\n",
    "\n",
    "        upload_or_update(LACTATE_MODEL_PATH, \"models/lactate_lightgbm_model.joblib\", \"Update lactate model\")\n",
    "        upload_or_update(RECOVERY_MODEL_PATH, \"models/recovery_lightgbm_model.joblib\", \"Update recovery model\")\n",
    "        upload_or_update(log_path, \"models/training_log.csv\", \"Update training log\")\n",
    "\n",
    "        print(\"üåê GitHub sync complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GitHub upload failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è GITHUB_TOKEN not set ‚Äì skipping GitHub upload.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
