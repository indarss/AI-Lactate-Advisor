{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a6722a",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  AI Lactate Training â€” **Complete Merged Notebook** (Lactate + Recovery)\n",
    "**Colab-ready, one-click pipeline.**  \n",
    "This notebook will:\n",
    "- âœ… Load existing models if found (`models/*.joblib`) â€” else **auto-train**\n",
    "- ðŸ©¸ Train **Lactate** model on wearable/effort data\n",
    "- ðŸ§¬ Train **Recovery** model on **biomarkers + wearables**\n",
    "- ðŸ’¡ Produce SHAP explainability plots for both\n",
    "- â˜ï¸ Upload models to **GitHub** (`indarss/AI-Lactate-Advisor`) with `GITHUB_TOKEN`\n",
    "- ðŸš€ Ping **Streamlit Cloud** to redeploy your app\n",
    "- ðŸ§ª If datasets are missing, it will **generate mock data** so the demo still runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74069880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (quiet)\n",
    "!pip install -q lightgbm shap PyGithub plotly joblib scikit-learn pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258904ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import shap, joblib\n",
    "from github import Github\n",
    "import plotly.express as px\n",
    "\n",
    "MODELS_DIR = \"models\"\n",
    "DATA_DIR = \"data\"\n",
    "pathlib.Path(MODELS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "pathlib.Path(DATA_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "LACTATE_MODEL_PATH = os.path.join(MODELS_DIR, \"lactate_lightgbm_model.joblib\")\n",
    "RECOVERY_MODEL_PATH = os.path.join(MODELS_DIR, \"recovery_lightgbm_model.joblib\")\n",
    "\n",
    "LACTATE_CSV = os.path.join(DATA_DIR, \"athlete_training_dataset_1000.csv\")\n",
    "BIOMARKER_CSV = os.path.join(DATA_DIR, \"athlete_training_dataset_with_biomarkers.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de044a0",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Datasets (or auto-generate if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthesize_lactate_csv(path=LACTATE_CSV, n=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": np.arange(0, n*10, 10),\n",
    "        \"heart_rate\": np.clip(np.linspace(110, 185, n) + np.random.normal(0, 3, n), 100, 190),\n",
    "        \"power\": np.clip(np.linspace(100, 400, n) + np.random.normal(0, 10, n), 0, 450),\n",
    "        \"cadence\": np.clip(np.linspace(75, 100, n) + np.random.normal(0, 2, n), 60, 120),\n",
    "        \"temperature\": np.linspace(18, 25, n) + np.random.normal(0, 0.2, n),\n",
    "        \"altitude\": np.linspace(100, 180, n) + np.random.normal(0, 1, n),\n",
    "    })\n",
    "    df[\"hr_slope_time\"] = df[\"heart_rate\"].diff() / df[\"time\"].diff()\n",
    "    df[\"hr_slope_power\"] = df[\"heart_rate\"].diff() / df[\"power\"].diff()\n",
    "    df[\"lactate\"] = (\n",
    "        1.3 +\n",
    "        0.015 * (df[\"heart_rate\"] - 120) +\n",
    "        0.008 * (df[\"power\"] - 150) / 10 +\n",
    "        3 * np.maximum(df[\"hr_slope_time\"], 0) +\n",
    "        np.random.normal(0, 0.2, n)\n",
    "    ).clip(1, 10)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def synthesize_biomarker_csv(path=BIOMARKER_CSV, n=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": np.arange(0, n*10, 10),\n",
    "        \"heart_rate\": np.clip(np.linspace(110, 185, n) + np.random.normal(0, 3, n), 100, 190),\n",
    "        \"power\": np.clip(np.linspace(100, 400, n) + np.random.normal(0, 10, n), 0, 450),\n",
    "        \"cadence\": np.clip(np.linspace(75, 100, n) + np.random.normal(0, 2, n), 60, 120),\n",
    "        \"temperature\": np.linspace(18, 25, n) + np.random.normal(0, 0.2, n),\n",
    "        \"altitude\": np.linspace(100, 180, n) + np.random.normal(0, 1, n),\n",
    "    })\n",
    "    df[\"hr_slope_time\"] = df[\"heart_rate\"].diff() / df[\"time\"].diff()\n",
    "    df[\"hr_slope_power\"] = df[\"heart_rate\"].diff() / df[\"power\"].diff()\n",
    "\n",
    "    # Biomarkers\n",
    "    df[\"CK\"] = np.random.normal(200, 50, n).clip(50, 600)\n",
    "    df[\"Cortisol\"] = np.random.normal(18, 4, n).clip(5, 35)\n",
    "    df[\"T_C_ratio\"] = np.random.normal(0.04, 0.01, n).clip(0.02, 0.08)\n",
    "    df[\"hsCRP\"] = np.random.normal(1.2, 0.5, n).clip(0.1, 5.0)\n",
    "    df[\"Glucose\"] = np.random.normal(90, 10, n).clip(70, 110)\n",
    "    df[\"RBC\"] = np.random.normal(4.8, 0.3, n).clip(4.0, 5.5)\n",
    "\n",
    "    df[\"lactate\"] = (\n",
    "        1.3 +\n",
    "        0.015 * (df[\"heart_rate\"] - 120) +\n",
    "        0.008 * (df[\"power\"] - 150) / 10 +\n",
    "        3 * np.maximum(df[\"hr_slope_time\"], 0) +\n",
    "        np.random.normal(0, 0.2, n)\n",
    "    ).clip(1, 10)\n",
    "\n",
    "    score = (\n",
    "        100\n",
    "        - 0.05 * (df[\"CK\"] - 200)\n",
    "        - 1.5 * (df[\"Cortisol\"] - 18)\n",
    "        - 1000 * (0.05 - df[\"T_C_ratio\"])\n",
    "        - 8 * (df[\"hsCRP\"] - 1)\n",
    "        - 0.2 * np.maximum(df[\"hr_slope_time\"], 0) * 1000\n",
    "        + 0.1 * (df[\"RBC\"] - 4.8) * 100\n",
    "    )\n",
    "    df[\"recovery_score\"] = np.clip(score, 0, 100)\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "# Load or synthesize\n",
    "if os.path.exists(LACTATE_CSV):\n",
    "    df_lac = pd.read_csv(LACTATE_CSV)\n",
    "    print(\"Loaded lactate dataset:\", df_lac.shape)\n",
    "else:\n",
    "    print(\"Lactate dataset missing â€” synthesizing...\")\n",
    "    df_lac = synthesize_lactate_csv()\n",
    "    print(\"Created:\", df_lac.shape, \"->\", LACTATE_CSV)\n",
    "\n",
    "if os.path.exists(BIOMARKER_CSV):\n",
    "    df_rec = pd.read_csv(BIOMARKER_CSV)\n",
    "    print(\"Loaded biomarker dataset:\", df_rec.shape)\n",
    "else:\n",
    "    print(\"Biomarker dataset missing â€” synthesizing...\")\n",
    "    df_rec = synthesize_biomarker_csv()\n",
    "    print(\"Created:\", df_rec.shape, \"->\", BIOMARKER_CSV)\n",
    "\n",
    "display(df_lac.head())\n",
    "display(df_rec.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c66dbf",
   "metadata": {},
   "source": [
    "## ðŸ©¸ Lactate Model â€” Load or Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lactate(df):\n",
    "    X = df.drop(columns=[\"lactate\"], errors=\"ignore\")\n",
    "    y = df[\"lactate\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Lactate RÂ²:\", round(r2_score(y_test, y_pred), 3))\n",
    "    print(\"Lactate MAE:\", round(mean_absolute_error(y_test, y_pred), 3))\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "if os.path.exists(LACTATE_MODEL_PATH):\n",
    "    try:\n",
    "        lactate_model = joblib.load(LACTATE_MODEL_PATH)\n",
    "        print(\"Loaded existing lactate model:\", LACTATE_MODEL_PATH)\n",
    "        X = df_lac.drop(columns=[\"lactate\"], errors=\"ignore\")\n",
    "        y = df_lac[\"lactate\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load existing model, retraining.\", e)\n",
    "        lactate_model, X_train, X_test, y_train, y_test = train_lactate(df_lac)\n",
    "        joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "        print(\"Saved:\", LACTATE_MODEL_PATH)\n",
    "else:\n",
    "    lactate_model, X_train, X_test, y_train, y_test = train_lactate(df_lac)\n",
    "    joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "    print(\"Saved:\", LACTATE_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3407d",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Lactate SHAP & 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer_lac = shap.Explainer(lactate_model, X_train.sample(min(1000, len(X_train)), random_state=42))\n",
    "shap_values_lac = explainer_lac(X_test.sample(min(500, len(X_test)), random_state=42))\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values_lac, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Lactate Model â€” SHAP Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_lac, x=\"heart_rate\", y=\"power\", z=\"lactate\",\n",
    "    color=\"lactate\", color_continuous_scale=\"Turbo\",\n",
    "    size=(\"hr_slope_time\" if \"hr_slope_time\" in df_lac.columns else None),\n",
    "    title=\"3D Relationship: Heart Rate Ã— Power Ã— Lactate\", opacity=0.85\n",
    ")\n",
    "fig.update_layout(scene=dict(xaxis_title='Heart Rate (bpm)', yaxis_title='Power (W)', zaxis_title='Lactate (mmol/L)'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855dca3",
   "metadata": {},
   "source": [
    "## ðŸ§¬ Recovery Model â€” Load or Train (Biomarkers + Wearables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_recovery(df):\n",
    "    Xr = df.drop(columns=[\"recovery_score\"], errors=\"ignore\")\n",
    "    yr = df[\"recovery_score\"]\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "    model = LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "    model.fit(Xr_train, yr_train)\n",
    "    yr_pred = model.predict(Xr_test)\n",
    "    print(\"Recovery RÂ²:\", round(r2_score(yr_test, yr_pred), 3))\n",
    "    print(\"Recovery MAE:\", round(mean_absolute_error(yr_test, yr_pred), 3))\n",
    "    return model, Xr_train, Xr_test, yr_train, yr_test\n",
    "\n",
    "if os.path.exists(RECOVERY_MODEL_PATH):\n",
    "    try:\n",
    "        recovery_model = joblib.load(RECOVERY_MODEL_PATH)\n",
    "        print(\"Loaded existing recovery model:\", RECOVERY_MODEL_PATH)\n",
    "        Xr = df_rec.drop(columns=[\"recovery_score\"], errors=\"ignore\")\n",
    "        yr = df_rec[\"recovery_score\"]\n",
    "        Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load existing recovery model, retraining.\", e)\n",
    "        recovery_model, Xr_train, Xr_test, yr_train, yr_test = train_recovery(df_rec)\n",
    "        joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "        print(\"Saved:\", RECOVERY_MODEL_PATH)\n",
    "else:\n",
    "    recovery_model, Xr_train, Xr_test, yr_train, yr_test = train_recovery(df_rec)\n",
    "    joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "    print(\"Saved:\", RECOVERY_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f865a",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Recovery SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer_rec = shap.Explainer(recovery_model, Xr_train.sample(min(1000, len(Xr_train)), random_state=42))\n",
    "shap_values_rec = explainer_rec(Xr_test.sample(min(500, len(Xr_test)), random_state=42))\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values_rec, Xr_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Recovery Model â€” SHAP Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf30e4",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(lactate_model, LACTATE_MODEL_PATH)\n",
    "joblib.dump(recovery_model, RECOVERY_MODEL_PATH)\n",
    "print(\"Saved:\", LACTATE_MODEL_PATH, \"and\", RECOVERY_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ef985",
   "metadata": {},
   "source": [
    "## â˜ï¸ Upload Both Models to GitHub (indarss/AI-Lactate-Advisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GITHUB_USERNAME = \"indarss\"\n",
    "GITHUB_REPO = \"AI-Lactate-Advisor\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")  # set in Colab: Runtime â†’ Secrets\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"âš ï¸ GITHUB_TOKEN not set. Skipping GitHub upload.\")\n",
    "else:\n",
    "    g = Github(GITHUB_TOKEN)\n",
    "    repo = g.get_user().get_repo(GITHUB_REPO)\n",
    "\n",
    "    def upload_or_update(local_path, repo_path, message):\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        try:\n",
    "            contents = repo.get_contents(repo_path)\n",
    "            repo.update_file(contents.path, message, content, contents.sha, branch=\"main\")\n",
    "            print(f\"âœ… Updated {repo_path}\")\n",
    "        except Exception as e:\n",
    "            repo.create_file(repo_path, message, content, branch=\"main\")\n",
    "            print(f\"âœ… Uploaded {repo_path}\")\n",
    "\n",
    "    upload_or_update(LACTATE_MODEL_PATH, \"models/lactate_lightgbm_model.joblib\", \"Update lactate model\")\n",
    "    upload_or_update(RECOVERY_MODEL_PATH, \"models/recovery_lightgbm_model.joblib\", \"Update recovery model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397096b3",
   "metadata": {},
   "source": [
    "## ðŸš€ Trigger Streamlit Cloud Redeploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "STREAMLIT_APP_URL = \"https://share.streamlit.io/indarss/AI-Lactate-Advisor/main/app.py\"\n",
    "print(\"Pinging Streamlit Cloud to trigger redeployâ€¦\")\n",
    "try:\n",
    "    r = requests.get(STREAMLIT_APP_URL, timeout=20)\n",
    "    print(\"Status:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        print(\"âœ… App reachable. Redeploy likely triggered automatically.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ App responded but may still be rebuilding.\")\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Could not ping Streamlit automatically. You can redeploy from Streamlit Cloud UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e0291",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### âœ… Youâ€™re done!\n",
    "- Models saved locally under `models/`\n",
    "- If `GITHUB_TOKEN` set, models synced to **indarss/AI-Lactate-Advisor**\n",
    "- Streamlit app pinged to redeploy\n",
    "\n",
    "> Tip: Keep your datasets under `data/` in your repo so the app and this notebook stay aligned.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
